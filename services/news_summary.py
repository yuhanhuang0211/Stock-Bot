import os
import logging
from urllib.parse import urlparse
from dotenv import load_dotenv
from googleapiclient.discovery import build
from newspaper import Article, Config
import requests

# LINE Bot SDK V3 Imports
from linebot.v3 import WebhookHandler
from linebot.v3.exceptions import InvalidSignatureError
from linebot.v3.messaging import (
    Configuration,
    ApiClient,
    MessagingApi,
    ReplyMessageRequest,
    TextMessage as V3TextMessage # Renamed to avoid conflict if any other TextMessage is used
)
from linebot.v3.webhooks import (
    MessageEvent,
    TextMessageContent # For type hinting and access
)

load_dotenv()

# --- ç’°å¢ƒè®Šæ•¸èˆ‡è¨­å®š ---
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_TOKEN")
LINE_CHANNEL_SECRET = os.getenv("LINE_SECRET")
SEARCH_API_KEY = os.getenv("SEARCH_API_KEY")
SEARCH_ENGINE_ID = os.getenv("SEARCH_ENGINE_ID")

assert LINE_CHANNEL_ACCESS_TOKEN, "ç¼ºå°‘ LINE_TOKEN (Channel Access Token)"
assert LINE_CHANNEL_SECRET, "ç¼ºå°‘ LINE_SECRET"
assert SEARCH_API_KEY, "ç¼ºå°‘ SEARCH_API_KEY"
assert SEARCH_ENGINE_ID, "ç¼ºå°‘ SEARCH_ENGINE_ID"

# --- LINE Bot API & Webhook åˆå§‹åŒ– (V3) ---
configuration = Configuration(access_token=LINE_CHANNEL_ACCESS_TOKEN)
api_client = ApiClient(configuration)
messaging_api = MessagingApi(api_client)        # ç™¼é€è¨Šæ¯
handler = WebhookHandler(LINE_CHANNEL_SECRET)   # è™•ç† webhook äº‹ä»¶

# --- è¨­å®šæ—¥èªŒ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# --- æ ¸å¿ƒé‚è¼¯ ---
def google_search_news(query):
    """æœå°‹æ–°èä¸¦è¿”å›ç¬¬ä¸€å€‹å¯è¨ªå•çš„URL"""
    try:
        service = build("customsearch", "v1", developerKey=SEARCH_API_KEY)
        res = service.cse().list(
            q=query,
            cx=SEARCH_ENGINE_ID,
            num=5,
        ).execute()

        if "items" not in res or not res["items"]:
            logger.info(f"Google æœå°‹ '{query}' æœªæ‰¾åˆ°é …ç›®ã€‚")
            return None, None

        # æ‰¾åˆ°ç¬¬ä¸€å€‹å¯è¨ªå•çš„é€£çµ
        for item in res["items"]:
            url = item.get("link")
            if not url:
                continue
            try:
                response = requests.head(url, timeout=5, allow_redirects=True)
                if response.status_code == 200:
                    logger.info(f"æ‰¾åˆ°å¯è¨ªå•æ–°è URL: {url}")
                    return url, item
                else:
                    logger.warning(f"URL {url} ç‹€æ…‹ç¢¼ {response.status_code}, è·³éã€‚")
            except requests.exceptions.RequestException as e:
                logger.warning(f"æª¢æŸ¥ URL {url} å¤±æ•—: {e}, è·³éã€‚")
                continue
        
        logger.warning(f"æœå°‹ '{query}' çš„çµæœéƒ½ç„¡æ³•è¨ªå•ã€‚")
        return None, None

    except Exception as e:
        logger.error(f"Google æœå°‹æ–°èéŒ¯èª¤: {e}")
        return None, None

def summarize_article(url, item=None):
    """ä½¿ç”¨ newspaper4k æå–æ–‡ç« æ‘˜è¦"""
    try:
        # newspaper4k é…ç½®
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'
        config.request_timeout = 15
        config.memoize_articles = False
        config.fetch_images = False
        config.language = 'zh'

        article = Article(url, config=config)
        article.download()
        article.parse()
        
        # newspaper4k çš„ NLP è™•ç†
        article.nlp()

        publish_date_str = None
        if article.publish_date:
            publish_date_str = article.publish_date.strftime('%Y-%m-%d')
        elif item:
            # å˜—è©¦å¾ meta æ¨™ç±¤ç²å–æ—¥æœŸ
            meta = item.get("pagemap", {}).get("metatags", [{}])[0]
            pd_value = meta.get("article:published_time") or meta.get("publishdate") or meta.get("pubdate")
            if pd_value:
                publish_date_str = pd_value.split('T')[0]

        return {
            "title": article.title or (item.get("title", "") if item else ""),
            "summary": article.summary or (item.get("snippet", "") if item else ""),
            "publish_date": publish_date_str,
            "source": urlparse(url).netloc,
            "url": url
        }

    except Exception as e:
        logger.error(f"æ–‡ç« æ‘˜è¦å¤±æ•— (URL: {url}): {e}")
        return {
            "title": item.get("title", "ç„¡æ³•å–å¾—æ¨™é¡Œ") if item else "ç„¡æ¨™é¡Œ",
            "summary": item.get("snippet", "ç„¡æ³•å–å¾—æ‘˜è¦") if item else "ç„¡æ‘˜è¦",
            "publish_date": None,
            "source": urlparse(url).netloc if url else "æœªçŸ¥ä¾†æº",
            "url": url,
            "error": str(e)
        }

def Google_search_news(query):
    """ä¸»è¦å°å¤–æ¥å£å‡½å¼ - å®Œæ•´çš„æ–°èæœå°‹å’Œæ‘˜è¦æµç¨‹"""
    try:
        logger.info(f"é–‹å§‹æœå°‹æ–°èé—œéµå­—: '{query}'")
        
        # æœå°‹æ–°è
        url, item = google_search_news(query)
        
        if url and item:
            # æå–æ–‡ç« æ‘˜è¦
            article_data = summarize_article(url, item)
            
            if "error" in article_data:
                logger.error(f"æ‘˜è¦æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {article_data['error']}")
                return f"æŠ±æ­‰ï¼Œè™•ç†ã€Œ{article_data['title']}ã€æ™‚é‡åˆ°å•é¡Œï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
            else:
                # æ ¼å¼åŒ–å›è¦†
                reply_parts = [
                    f"ğŸ“° {article_data['title']}",
                    (f"ğŸ“… ç™¼å¸ƒ: {article_data['publish_date']}" if article_data['publish_date'] else "ğŸ“… ç™¼å¸ƒæ—¥æœŸæœªçŸ¥"),
                    f"ğŸ” ä¾†æº: {article_data['source']}",
                    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
                    f"ğŸ“„ æ–°èæ‘˜è¦:\n{article_data['summary']}",
                    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
                    f"ğŸ”— å®Œæ•´æ–°è: {article_data['url']}"
                ]
                result = "\n\n".join(part for part in reply_parts if part)
                logger.info(f"æˆåŠŸè™•ç†æ–°èæŸ¥è©¢: {query}")
                return result
        else:
            logger.info(f"æ‰¾ä¸åˆ°èˆ‡ '{query}' ç›¸é—œçš„æ–°è")
            return f"æŠ±æ­‰ï¼Œæ‰¾ä¸åˆ°èˆ‡ã€Œ{query}ã€ç›¸é—œçš„æ–°èï¼Œè«‹æ›å€‹é—œéµå­—å†è©¦çœ‹çœ‹ï¼"
            
    except Exception as e:
        logger.error(f"æ–°èæœå°‹è™•ç†éŒ¯èª¤: {e}")
        return "æŠ±æ­‰ï¼Œæ–°èæŸ¥è©¢åŠŸèƒ½æš«æ™‚ç„¡æ³•ä½¿ç”¨ã€‚"

if __name__ == "__main__":
    # æ¸¬è©¦åŠŸèƒ½
    test_query = "å°ç©é›»è‚¡åƒ¹"
    result = Google_search_news(test_query)
    print("æ¸¬è©¦çµæœï¼š")
    print(result)
