import os
import logging
from urllib.parse import urlparse
from dotenv import load_dotenv
from googleapiclient.discovery import build
from newspaper import Article, Config as NewspaperConfig # newspaper4k ä¹Ÿä½¿ç”¨ Config
import requests

load_dotenv()

# --- ç’°å¢ƒè®Šæ•¸èˆ‡åˆå§‹åŒ– ---
SEARCH_API_KEY = os.getenv("SEARCH_API_KEY")
SEARCH_ENGINE_ID = os.getenv("SEARCH_ENGINE_ID")

assert SEARCH_API_KEY, "ç¼ºå°‘ SEARCH_API_KEYï¼Œnews_summary æ¨¡çµ„ç„¡æ³•é‹è¡Œ"
assert SEARCH_ENGINE_ID, "ç¼ºå°‘ SEARCH_ENGINE_IDï¼Œnews_summary æ¨¡çµ„ç„¡æ³•é‹è¡Œ"

# è¨˜éŒ„å™¨è¨­å®šï¼ˆèˆ‡å…¶ä»–æ¨¡çµ„ä¸€è‡´ï¼‰
logger = logging.getLogger(__name__)
if not logger.handlers: # è‹¥æ¨¡çµ„è¢«å¤šæ¬¡åŒ¯å…¥ï¼Œé¿å…é‡è¤‡æ·»åŠ  handler
    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(module)s - %(funcName)s - %(message)s')


# --- æ ¸å¿ƒé‚è¼¯ ---
def _Google_Search_api_call(query: str):
    """å…§éƒ¨å‡½å¼ï¼šå‘¼å« Google Custom Search APIã€‚"""
    service = build("customsearch", "v1", developerKey=SEARCH_API_KEY)
    res = service.cse().list(
        q=query,
        cx=SEARCH_ENGINE_ID,
        num=5, # æŠ“å–æ•¸ç­†çµæœä»¥æ‰¾åˆ°å¯ç”¨çš„é€£çµ
        # è‹¥ CSE æœ‰è¨­ç½®ï¼Œå¯è€ƒæ…®åŠ ä¸Š dateRestrict="d7" æˆ– sort="date"
    ).execute()
    return res

def _get_first_accessible_url(query_for_log: str, search_results_items: list | None):
    """å…§éƒ¨å‡½å¼ï¼šå¾æœå°‹çµæœä¸­æ‰¾å‡ºç¬¬ä¸€å€‹å¯ä»¥æ­£å¸¸é€£ç·šçš„ç¶²å€ã€‚"""
    if not search_results_items:
        return None, None
        
    for item in search_results_items:
        url = item.get("link")
        if not url:
            continue
        try:
            response = requests.head(url, timeout=5, allow_redirects=True)
            if response.status_code == 200:
                logger.info(f"æŸ¥è©¢ã€Œ{query_for_log}ã€æ‰¾åˆ°å¯ç”¨é€£çµï¼š{url}")
                return url, item # å›å‚³ç¶²å€èˆ‡è©²é …ç›®ä»¥å–å¾—å¾ŒçºŒè³‡è¨Š
            else:
                logger.warning(f"é€£çµ {url}ï¼ˆæŸ¥è©¢ï¼š{query_for_log}ï¼‰å›æ‡‰ç¢¼ç‚º {response.status_code}ï¼Œç•¥éã€‚")
        except requests.exceptions.RequestException as e:
            logger.warning(f"æª¢æŸ¥é€£çµ {url}ï¼ˆæŸ¥è©¢ï¼š{query_for_log}ï¼‰å¤±æ•—ï¼š{e}ï¼Œç•¥éã€‚")
            continue
    return None, None

def _summarize_article_with_newspaper(url: str, item_from_search=None) -> dict:
    """å…§éƒ¨å‡½å¼ï¼šä½¿ç”¨ newspaper4k æ“·å–èˆ‡æ‘˜è¦æ–‡ç« å…§å®¹ã€‚"""
    try:
        # newspaper4k è¨­å®š
        config = NewspaperConfig()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'
        config.request_timeout = 15  # ç§’æ•¸
        config.memoize_articles = False # ä¸å¿«å–æ–‡ç« 
        config.fetch_images = False    # ä¸ä¸‹è¼‰åœ–ç‰‡
        config.language = 'zh'         # æŒ‡å®šèªè¨€ä»¥æ”¹å–„è‡ªç„¶èªè¨€è™•ç†æ•ˆæœ

        article = Article(url, config=config)
        article.download()
        article.parse()
        article.nlp() # åŸ·è¡Œ NLP åˆ†æï¼ˆåŒ…æ‹¬æ‘˜è¦ï¼‰

        publish_date_str = None
        if article.publish_date:
            publish_date_str = article.publish_date.strftime('%Y-%m-%d')
        elif item_from_search: # è‹¥ç„¡å‰‡å›é€€ä½¿ç”¨æœå°‹çµæœä¸­çš„æ—¥æœŸ
            meta = item_from_search.get("pagemap", {}).get("metatags", [{}])[0]
            pd_value = meta.get("article:published_time") or meta.get("publishdate") or meta.get("pubdate")
            if pd_value:
                publish_date_str = pd_value.split('T')[0]

        return {
            "title": article.title or (item_from_search.get("title", "N/A") if item_from_search else "N/A"),
            "summary": article.summary or (item_from_search.get("snippet", "N/A") if item_from_search else "N/A"), # newspaper4k æä¾›çš„æ‘˜è¦
            "publish_date": publish_date_str,
            "source": urlparse(url).netloc,
            "url": url
        }
    except Exception as e:
        logger.error(f"è™•ç†ç¶²å€ {url} æ™‚ï¼Œnewspaper4k ç™¼ç”ŸéŒ¯èª¤ï¼š{e}", exc_info=True)
        title = item_from_search.get("title", "ç„¡æ³•å–å¾—æ¨™é¡Œ") if item_from_search else "æ¨™é¡Œè™•ç†éŒ¯èª¤"
        summary = item_from_search.get("snippet", "ç„¡æ³•å–å¾—æ‘˜è¦ (newspaper4k)") if item_from_search else "æ‘˜è¦è™•ç†éŒ¯èª¤"
        return {
            "title": title, "summary": summary, "publish_date": None,
            "source": urlparse(url).netloc if url else "æœªçŸ¥ä¾†æº", "url": url, "error": str(e)
        }

# --- æä¾›çµ¦ app.py ä½¿ç”¨çš„å…¬é–‹å‡½å¼ ---
def get_news_summary(query: str) -> str:
    """
    æ ¹æ“šè¼¸å…¥æŸ¥è©¢æ–°èï¼Œä¸¦å›å‚³æ ¼å¼åŒ–å¾Œçš„æ‘˜è¦å­—ä¸²ã€‚
    ä½¿ç”¨ newspaper4k åŸ·è¡Œæ‘˜è¦è™•ç†ã€‚
    """
    try:
        logger.info(f"æ¥æ”¶åˆ°æ–°èæ‘˜è¦è«‹æ±‚ï¼š'{query}'")
        search_response = _Google_Search_api_call(query)

        if "items" not in search_response or not search_response["items"]:
            logger.info(f"Google æœå°‹ã€Œ{query}ã€ï¼ˆæ–°èï¼‰æœªæ‰¾åˆ°çµæœã€‚")
            return f"æŠ±æ­‰ï¼Œæ‰¾ä¸åˆ°èˆ‡ã€Œ{query}ã€ç›¸é—œçš„æ–°èã€‚"

        url, item = _get_first_accessible_url(query, search_response["items"])
        
        if not url or not item:
            logger.warning(f"æ–°èæŸ¥è©¢ã€Œ{query}ã€æœªæ‰¾åˆ°å¯è®€å–çš„é€£çµã€‚")
            return f"æŠ±æ­‰ï¼Œç›®å‰æ‰¾ä¸åˆ°ã€Œ{query}ã€ç›¸é—œä¸”å¯è®€å–çš„æ–°èå ±å°ã€‚"

        article_data = _summarize_article_with_newspaper(url, item)
        
        if "error" in article_data and not article_data.get("summary", "").strip():
             return f"æŠ±æ­‰ï¼Œè™•ç†æ–°èã€Œ{article_data.get('title','æœªçŸ¥æ¨™é¡Œ')}ã€æ™‚é‡åˆ°å•é¡Œã€‚"

        # æ ¼å¼åŒ–å›è¦†å­—ä¸²
        reply_parts = [
            f"ğŸ“° {article_data.get('title', 'ç„¡æ¨™é¡Œ')}",
            (f"ğŸ“… ç™¼å¸ƒ: {article_data['publish_date']}" if article_data['publish_date'] else "ğŸ“… ç™¼å¸ƒæ—¥æœŸæœªçŸ¥"),
            f"ğŸ” ä¾†æº: {article_data.get('source', 'æœªçŸ¥')}",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
            f"ğŸ“„ æ–°èæ‘˜è¦:\n{article_data.get('summary', 'ç„¡æ³•ç”¢ç”Ÿæ‘˜è¦ã€‚')}",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
            f"ğŸ”— å®Œæ•´æ–°è: {article_data.get('url', '#')}"
        ]
        result_string = "\n\n".join(part for part in reply_parts if part)
        logger.info(f"æˆåŠŸè™•ç†æ–°èæŸ¥è©¢ã€Œ{query}ã€ã€‚æ¨™é¡Œï¼š{article_data.get('title')}")
        return result_string
            
    except Exception as e:
        logger.error(f"get_news_summary è™•ç†æŸ¥è©¢ã€Œ{query}ã€æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}", exc_info=True)
        return "æŠ±æ­‰ï¼Œæ–°èæŸ¥è©¢æœå‹™ç›®å‰é‡åˆ°ä¸€äº›å•é¡Œï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

if __name__ == "__main__":
    logger.info("æ¸¬è©¦ news_summary.py...")
    test_query = "å°ç©é›»è‚¡åƒ¹"
    summary = get_news_summary(test_query)
    print(f"\n--- æ¸¬è©¦æŸ¥è©¢ã€Œ{test_query}ã€ ---")
    print(summary)
